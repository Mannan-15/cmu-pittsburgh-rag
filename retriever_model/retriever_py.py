# -*- coding: utf-8 -*-
"""cmuass2_retriever.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171wZwAEQ3XAswKZJHdG2DZl1a3MXRf13
"""

pip install langchain chromadb sentence-transformers langchain-community

from google.colab import drive
drive.mount('/content/drive')

import json

try :
  with open("chunks.json", 'r', encoding = 'utf-8') as f :
    chunks = json.load(f)
except :
  print("error")

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

data = [Document(metadata = chunk['metadata'], page_content = chunk['page_content']) for chunk in chunks]
len(data)

# Option 1: A standard, fast, and excellent baseline
# model_name = "sentence-transformers/all-MiniLM-L6-v2"

# Option 2: DPR (Dense Passage Retrieval)
# model_name = "facebook/dpr-ctx_encoder-multiset-base"

# Option 3: Contriever
model_name = "facebook/contriever-msmarco"

embedding_model = HuggingFaceEmbeddings(
    model_name = model_name,
    model_kwargs = {'device': 'cpu'},
    # encode_kwargs
)

db_path = r"G:\My Drive\CMU assign 2\cmu-pittsburgh-rag"
vector_store = Chroma.from_documents(
    documents = data,
    embedding = embedding_model,
    persist_directory = db_path,
)

retriever = vector_store.as_retriever(search_kwargs = {'k'  : 3})

# --- Test your retriever ---
query = "What happened in Pittsburgh in 1845?"
retrieved_docs = retriever.invoke(query)

print(f"\nQuery: {query}")
print("\n--- Retrieved Docs ---")
for i, doc in enumerate(retrieved_docs):
    print(f"--- DOC {i+1} ---")
    print(f"Content: {doc.page_content[:150]}...")
    print(f"Source: {doc.metadata.get('source')}\n")

"""__Loading database__"""

from google.colab import drive
drive.mount('/content/drive')

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

persist_path = r"/content/drive/MyDrive/CMU assign 2/cmu-pittsburgh-rag/G:/My Drive/CMU assign 2/cmu-pittsburgh-rag"
embedding_model = HuggingFaceBgeEmbeddings(
    model_name = "facebook/contriever-msmarco",
    model_kwargs = {'device': 'cpu'}
)

print(f"Loading database from: {persist_path}")
db = Chroma(
    persist_directory = persist_path,
    embedding_function = embedding_model
)

print("âœ… Database loaded successfully!")

retriever = db.as_retriever()

# --- Test your retriever ---
query = "What happened in Pittsburgh in 1845?"
retrieved_docs = retriever.invoke(query)

print(f"\nQuery: {query}")
print("\n--- Retrieved Docs ---")
for i, doc in enumerate(retrieved_docs):
    print(f"--- DOC {i+1} ---")
    print(f"Content: {doc.page_content[:150]}...")
    print(f"Source: {doc.metadata.get('source')}\n")